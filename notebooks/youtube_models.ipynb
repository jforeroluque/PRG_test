{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<center><h1>An√°lisis Posicionamiento Youtube</h1></center>\n",
    "\n",
    "___\n",
    "\n",
    "<center><h2>Pragma</h2></center><br>\n",
    "<center> Assessment Consultor Cientifico de Datos </center>\n",
    "\n",
    "___\n",
    "<p></p>\n",
    "<center style=\"color: #AA6373; font-weight: 400;\"><strong>Presentado por:</strong></center>\n",
    "<center style=\"color: #AA6373; font-weight: 400;\">Jorge Forero L.</center>\n",
    "\n",
    "<center>Mayo 2025</center>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Common Modules ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cleaned_youtube_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cleaned_youtube_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Time-based features\n",
    "df['publish_hour'] = pd.to_datetime(df['publish_time']).dt.hour\n",
    "df['publish_day'] = pd.to_datetime(df['publish_time']).dt.day_name()\n",
    "df['days_to_trend'] = (pd.to_datetime(df['trending_date']) - pd.to_datetime(df['publish_time'])).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Engagement metrics\n",
    "df['engagement_rate'] = (df['likes'] + df['dislikes'] + df['comment_count']) / df['views']\n",
    "df['like_ratio'] = df['likes'] / (df['likes'] + df['dislikes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Development ---\n",
    "# 1. Define features and target\n",
    "features = ['category_id', 'publish_hour', 'days_to_trend', 'comments_disabled', \n",
    "           'ratings_disabled', 'video_error_or_removed']\n",
    "target = 'views'  # or 'engagement_rate' depending on the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split the data\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create preprocessing pipeline\n",
    "numeric_features = ['publish_hour', 'days_to_trend']\n",
    "categorical_features = ['category_id', 'comments_disabled', 'ratings_disabled', 'video_error_or_removed']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Pipeline\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Training and Evaluation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'MSE': mean_squared_error(y_test, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'R2': r2_score(y_test, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Feature Importance Analysis\n",
    "# For Random Forest and XGBoost models\n",
    "feature_importance = {}\n",
    "for name, model in models.items():\n",
    "    if name in ['Random Forest', 'XGBoost']:\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Get feature names after preprocessing\n",
    "        numeric_feature_names = numeric_features\n",
    "        categorical_feature_names = pipeline.named_steps['preprocessor']\\\n",
    "            .named_transformers_['cat']\\\n",
    "            .get_feature_names_out(categorical_features)\n",
    "        \n",
    "        # Combine feature names properly using numpy concatenate\n",
    "        feature_names = np.concatenate([numeric_feature_names, categorical_feature_names])\n",
    "        \n",
    "        # Get feature importance\n",
    "        if name == 'Random Forest':\n",
    "            importance = pipeline.named_steps['model'].feature_importances_\n",
    "        else:\n",
    "            importance = pipeline.named_steps['model'].feature_importances_\n",
    "        \n",
    "        feature_importance[name] = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../models/best_model.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m best_pipeline.fit(X_train, y_train)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../models/best_model.joblib\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/JFCL Projects/PRG_test/venv/lib/python3.11/site-packages/joblib/numpy_pickle.py:599\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(value, filename, compress, protocol)\u001b[39m\n\u001b[32m    597\u001b[39m         NumpyPickler(f, protocol=protocol).dump(value)\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    600\u001b[39m         NumpyPickler(f, protocol=protocol).dump(value)\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../models/best_model.joblib'"
     ]
    }
   ],
   "source": [
    "# 7. Model Persistence\n",
    "# Save the best performing model\n",
    "best_model_name = min(results, key=lambda x: results[x]['RMSE'])\n",
    "best_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', models[best_model_name])\n",
    "])\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_pipeline, '../models/best_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
